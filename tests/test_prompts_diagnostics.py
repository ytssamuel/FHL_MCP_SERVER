"""
FHL Bible MCP Server - Prompts è¨ºæ–·æ¸¬è©¦å¥—ä»¶

å®Œæ•´çš„ Prompts è¨ºæ–·æ¸¬è©¦ï¼Œç”¨æ–¼è­˜åˆ¥æ‰€æœ‰å•é¡Œä¸¦ç”Ÿæˆè¨ºæ–·å ±å‘Š
"""

import pytest
import sys
import os
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
import json

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.fhl_bible_mcp.prompts.manager import PromptManager
from src.fhl_bible_mcp.prompts.base import PromptTemplate


# ==================== æ¸¬è©¦é…ç½® ====================

# é•·åº¦æ¨™æº–ï¼ˆåš´æ ¼ï¼‰
LENGTH_STANDARDS = {
    'basic': 500,      # åŸºç¤é¡
    'reading': 700,    # è®€ç¶“é¡
    'study': 800,      # ç ”ç¶“é¡
    'special': 900,    # ç‰¹æ®Šé¡
    'advanced': 1000   # é€²éšé¡
}

# æ‰€æœ‰ Prompts çš„å®Œæ•´æ¸…å–®ï¼ˆ19 å€‹ï¼‰
ALL_PROMPTS = [
    # Basic (4)
    ('basic', 'basic_help_guide', 'BasicHelpGuidePrompt'),
    ('basic', 'basic_uri_demo', 'BasicURIDemoPrompt'),
    ('basic', 'basic_quick_lookup', 'BasicQuickLookupPrompt'),
    ('basic', 'basic_tool_reference', 'BasicToolReferencePrompt'),
    
    # Reading (3)
    ('reading', 'reading_daily', 'ReadingDailyPrompt'),
    ('reading', 'reading_chapter', 'ReadingChapterPrompt'),
    ('reading', 'reading_passage', 'ReadingPassagePrompt'),
    
    # Study (4)
    ('study', 'study_verse_deep', 'StudyVerseDeepPrompt'),
    ('study', 'study_topic_deep', 'StudyTopicDeepPrompt'),
    ('study', 'study_translation_compare', 'StudyTranslationComparePrompt'),
    ('study', 'study_word_original', 'StudyWordOriginalPrompt'),
    
    # Special (5)
    ('special', 'special_sermon_prep', 'SpecialSermonPrepPrompt'),
    ('special', 'special_devotional', 'SpecialDevotionalPrompt'),
    ('special', 'special_memory_verse', 'SpecialMemoryVersePrompt'),
    ('special', 'special_topical_chain', 'SpecialTopicalChainPrompt'),
    ('special', 'special_bible_trivia', 'SpecialBibleTriviaPrompt'),
    
    # Advanced (3)
    ('advanced', 'advanced_cross_reference', 'AdvancedCrossReferencePrompt'),
    ('advanced', 'advanced_parallel_gospels', 'AdvancedParallelGospelsPrompt'),
    ('advanced', 'advanced_character_study', 'AdvancedCharacterStudyPrompt'),
]


@dataclass
class PromptDiagnosticResult:
    """Prompt è¨ºæ–·çµæœ"""
    category: str
    name: str
    class_name: str
    import_success: bool
    import_error: str = ""
    instantiate_success: bool = False
    instantiate_error: str = ""
    render_success: bool = False
    render_error: str = ""
    rendered_length: int = 0
    length_standard: int = 0
    length_over: int = 0
    length_status: str = "UNKNOWN"  # PASS / FAIL / ERROR
    has_clear_steps: bool = False
    step_count: int = 0
    has_action_verbs: bool = False
    structure_score: int = 0  # 0-100
    structure_notes: str = ""


class PromptsDiagnostics:
    """Prompts è¨ºæ–·æ¸¬è©¦é¡"""
    
    def __init__(self):
        self.results: List[PromptDiagnosticResult] = []
        self.manager = None
    
    # ==================== æ¸¬è©¦ 1: å°å…¥æ¸¬è©¦ ====================
    
    def test_import_all_prompts(self) -> Tuple[int, int, List[PromptDiagnosticResult]]:
        """
        æ¸¬è©¦ 1: æ‰€æœ‰ prompts èƒ½å¦æ­£ç¢ºå°å…¥
        
        Returns:
            (æˆåŠŸæ•¸, å¤±æ•—æ•¸, çµæœåˆ—è¡¨)
        """
        print("\n" + "="*80)
        print("æ¸¬è©¦ 1: Prompts å°å…¥æ¸¬è©¦")
        print("="*80)
        
        success_count = 0
        fail_count = 0
        results = []
        
        for category, name, class_name in ALL_PROMPTS:
            result = PromptDiagnosticResult(
                category=category,
                name=name,
                class_name=class_name,
                import_success=False,
                length_standard=LENGTH_STANDARDS[category]
            )
            
            try:
                # å˜—è©¦å°å…¥
                module_path = f"src.fhl_bible_mcp.prompts.{category}"
                module = __import__(module_path, fromlist=[class_name])
                prompt_class = getattr(module, class_name)
                
                result.import_success = True
                success_count += 1
                print(f"âœ… {name}: å°å…¥æˆåŠŸ")
                
            except Exception as e:
                result.import_success = False
                result.import_error = str(e)
                fail_count += 1
                print(f"âŒ {name}: å°å…¥å¤±æ•— - {e}")
            
            results.append(result)
        
        print(f"\nğŸ“Š å°å…¥æ¸¬è©¦çµæœ: {success_count}/{len(ALL_PROMPTS)} æˆåŠŸ")
        self.results = results
        return success_count, fail_count, results
    
    # ==================== æ¸¬è©¦ 2: å¯¦ä¾‹åŒ–æ¸¬è©¦ ====================
    
    def test_instantiate_all_prompts(self) -> Tuple[int, int]:
        """
        æ¸¬è©¦ 2: æ‰€æœ‰ prompts èƒ½å¦å¯¦ä¾‹åŒ–
        
        Returns:
            (æˆåŠŸæ•¸, å¤±æ•—æ•¸)
        """
        print("\n" + "="*80)
        print("æ¸¬è©¦ 2: Prompts å¯¦ä¾‹åŒ–æ¸¬è©¦")
        print("="*80)
        
        success_count = 0
        fail_count = 0
        
        for result in self.results:
            if not result.import_success:
                print(f"â­ï¸  {result.name}: è·³éï¼ˆå°å…¥å¤±æ•—ï¼‰")
                continue
            
            try:
                # å˜—è©¦å¯¦ä¾‹åŒ–
                module_path = f"src.fhl_bible_mcp.prompts.{result.category}"
                module = __import__(module_path, fromlist=[result.class_name])
                prompt_class = getattr(module, result.class_name)
                
                # ç„¡åƒæ•¸å¯¦ä¾‹åŒ–
                instance = prompt_class()
                
                result.instantiate_success = True
                success_count += 1
                print(f"âœ… {result.name}: å¯¦ä¾‹åŒ–æˆåŠŸ")
                
            except Exception as e:
                result.instantiate_success = False
                result.instantiate_error = str(e)
                fail_count += 1
                print(f"âŒ {result.name}: å¯¦ä¾‹åŒ–å¤±æ•— - {e}")
        
        print(f"\nğŸ“Š å¯¦ä¾‹åŒ–æ¸¬è©¦çµæœ: {success_count}/{len([r for r in self.results if r.import_success])} æˆåŠŸ")
        return success_count, fail_count
    
    # ==================== æ¸¬è©¦ 3: æ¸²æŸ“æ¸¬è©¦ ====================
    
    def test_render_all_prompts(self) -> Tuple[int, int]:
        """
        æ¸¬è©¦ 3: æ‰€æœ‰ prompts èƒ½å¦æ­£å¸¸æ¸²æŸ“
        
        Returns:
            (æˆåŠŸæ•¸, å¤±æ•—æ•¸)
        """
        print("\n" + "="*80)
        print("æ¸¬è©¦ 3: Prompts æ¸²æŸ“æ¸¬è©¦")
        print("="*80)
        
        success_count = 0
        fail_count = 0
        
        for result in self.results:
            if not result.import_success or not result.instantiate_success:
                print(f"â­ï¸  {result.name}: è·³éï¼ˆå°å…¥æˆ–å¯¦ä¾‹åŒ–å¤±æ•—ï¼‰")
                continue
            
            try:
                # å˜—è©¦æ¸²æŸ“
                module_path = f"src.fhl_bible_mcp.prompts.{result.category}"
                module = __import__(module_path, fromlist=[result.class_name])
                prompt_class = getattr(module, result.class_name)
                instance = prompt_class()
                
                # ä½¿ç”¨é»˜èªåƒæ•¸æ¸²æŸ“
                rendered = instance.render()
                
                if rendered and isinstance(rendered, str):
                    result.render_success = True
                    result.rendered_length = len(rendered)
                    success_count += 1
                    print(f"âœ… {result.name}: æ¸²æŸ“æˆåŠŸ ({result.rendered_length} å­—)")
                else:
                    result.render_success = False
                    result.render_error = "æ¸²æŸ“è¿”å›ç©ºæˆ–éå­—ç¬¦ä¸²"
                    fail_count += 1
                    print(f"âŒ {result.name}: æ¸²æŸ“å¤±æ•— - è¿”å›ç©ºæˆ–éå­—ç¬¦ä¸²")
                
            except Exception as e:
                result.render_success = False
                result.render_error = str(e)
                fail_count += 1
                print(f"âŒ {result.name}: æ¸²æŸ“å¤±æ•— - {e}")
        
        print(f"\nğŸ“Š æ¸²æŸ“æ¸¬è©¦çµæœ: {success_count}/{len([r for r in self.results if r.import_success and r.instantiate_success])} æˆåŠŸ")
        return success_count, fail_count
    
    # ==================== æ¸¬è©¦ 4: é•·åº¦åˆ†æ ====================
    
    def test_prompts_length_analysis(self) -> Dict[str, Any]:
        """
        æ¸¬è©¦ 4: é•·åº¦åˆ†æ
        
        Returns:
            çµ±è¨ˆæ•¸æ“šå­—å…¸
        """
        print("\n" + "="*80)
        print("æ¸¬è©¦ 4: Prompts é•·åº¦åˆ†æ")
        print("="*80)
        
        pass_count = 0
        fail_count = 0
        total_length = 0
        max_length = 0
        max_length_prompt = ""
        
        # æŒ‰é¡åˆ¥çµ±è¨ˆ
        category_stats = {
            'basic': {'count': 0, 'total': 0, 'pass': 0, 'fail': 0},
            'reading': {'count': 0, 'total': 0, 'pass': 0, 'fail': 0},
            'study': {'count': 0, 'total': 0, 'pass': 0, 'fail': 0},
            'special': {'count': 0, 'total': 0, 'pass': 0, 'fail': 0},
            'advanced': {'count': 0, 'total': 0, 'pass': 0, 'fail': 0},
        }
        
        print(f"\n{'Prompt':<35} {'é•·åº¦':<8} {'æ¨™æº–':<8} {'è¶…æ¨™':<8} {'ç‹€æ…‹':<8}")
        print("-" * 75)
        
        for result in self.results:
            if not result.render_success:
                result.length_status = "ERROR"
                print(f"{result.name:<35} {'N/A':<8} {result.length_standard:<8} {'N/A':<8} âŒ ERROR")
                continue
            
            # è¨ˆç®—è¶…æ¨™é‡
            result.length_over = result.rendered_length - result.length_standard
            
            # åˆ¤æ–·ç‹€æ…‹
            if result.rendered_length <= result.length_standard:
                result.length_status = "PASS"
                pass_count += 1
                category_stats[result.category]['pass'] += 1
                status_icon = "âœ… PASS"
            else:
                result.length_status = "FAIL"
                fail_count += 1
                category_stats[result.category]['fail'] += 1
                status_icon = "âŒ FAIL"
            
            # çµ±è¨ˆ
            total_length += result.rendered_length
            category_stats[result.category]['count'] += 1
            category_stats[result.category]['total'] += result.rendered_length
            
            if result.rendered_length > max_length:
                max_length = result.rendered_length
                max_length_prompt = result.name
            
            # æ‰“å°çµæœ
            print(f"{result.name:<35} {result.rendered_length:<8} {result.length_standard:<8} {result.length_over if result.length_over > 0 else '-':<8} {status_icon}")
        
        # è¨ˆç®—å¹³å‡å€¼
        render_success_count = len([r for r in self.results if r.render_success])
        avg_length = total_length / render_success_count if render_success_count > 0 else 0
        
        # æ‰“å°çµ±è¨ˆ
        print("\n" + "="*80)
        print("ğŸ“Š é•·åº¦çµ±è¨ˆ")
        print("="*80)
        print(f"ç¸½è¨ˆ: {render_success_count} å€‹ prompts")
        print(f"é€šé: {pass_count} å€‹ ({pass_count/render_success_count*100:.1f}%)")
        print(f"æœªé€šé: {fail_count} å€‹ ({fail_count/render_success_count*100:.1f}%)")
        print(f"å¹³å‡é•·åº¦: {avg_length:.0f} å­—")
        print(f"æœ€é•· Prompt: {max_length_prompt} ({max_length} å­—)")
        
        print("\næŒ‰é¡åˆ¥çµ±è¨ˆ:")
        print("-" * 75)
        print(f"{'é¡åˆ¥':<12} {'æ•¸é‡':<8} {'å¹³å‡é•·åº¦':<12} {'é€šé':<8} {'æœªé€šé':<8}")
        print("-" * 75)
        for category, stats in category_stats.items():
            if stats['count'] > 0:
                avg = stats['total'] / stats['count']
                print(f"{category:<12} {stats['count']:<8} {avg:<12.0f} {stats['pass']:<8} {stats['fail']:<8}")
        
        return {
            'total': render_success_count,
            'pass': pass_count,
            'fail': fail_count,
            'avg_length': avg_length,
            'max_length': max_length,
            'max_length_prompt': max_length_prompt,
            'category_stats': category_stats
        }
    
    # ==================== æ¸¬è©¦ 5: çµæ§‹æª¢æŸ¥ ====================
    
    def test_prompts_structure_check(self) -> Dict[str, Any]:
        """
        æ¸¬è©¦ 5: çµæ§‹æª¢æŸ¥
        
        æª¢æŸ¥ prompt æ˜¯å¦åŒ…å«æ¸…æ™°çš„æ­¥é©Ÿã€å‹•è©é–‹é ­çš„æŒ‡ä»¤ç­‰
        
        Returns:
            çµ±è¨ˆæ•¸æ“šå­—å…¸
        """
        print("\n" + "="*80)
        print("æ¸¬è©¦ 5: Prompts çµæ§‹æª¢æŸ¥")
        print("="*80)
        
        action_verbs = [
            'æŸ¥è©¢', 'æœå°‹', 'åˆ†æ', 'æ¯”è¼ƒ', 'åˆ—å‡º', 'æå–', 'ç¸½çµ', 'æ­¸ç´',
            'æª¢æŸ¥', 'é©—è­‰', 'è¨ˆç®—', 'çµ±è¨ˆ', 'æ’åº', 'ç¯©é¸', 'çµ„ç¹”', 'å»ºç«‹',
            'å‰µå»º', 'ç”Ÿæˆ', 'è¼¸å‡º', 'é¡¯ç¤º', 'å±•ç¤º', 'ä½¿ç”¨', 'åŸ·è¡Œ', 'èª¿ç”¨'
        ]
        
        good_structure = 0
        poor_structure = 0
        
        print(f"\n{'Prompt':<35} {'æ­¥é©Ÿ':<8} {'å‹•è©':<8} {'è©•åˆ†':<8} {'ç‹€æ…‹':<10}")
        print("-" * 75)
        
        for result in self.results:
            if not result.render_success:
                print(f"{result.name:<35} {'N/A':<8} {'N/A':<8} {'N/A':<8} â­ï¸  ERROR")
                continue
            
            # ç²å–æ¸²æŸ“å…§å®¹
            try:
                module_path = f"src.fhl_bible_mcp.prompts.{result.category}"
                module = __import__(module_path, fromlist=[result.class_name])
                prompt_class = getattr(module, result.class_name)
                instance = prompt_class()
                rendered = instance.render()
                
                # æª¢æŸ¥æ­¥é©Ÿæ¨™è¨˜
                step_markers = ['## æ­¥é©Ÿ', 'æ­¥é©Ÿ 1', 'æ­¥é©Ÿ 2', 'æ­¥é©Ÿ 3', 'Step 1', 'Step 2']
                result.has_clear_steps = any(marker in rendered for marker in step_markers)
                
                # è¨ˆç®—æ­¥é©Ÿæ•¸
                result.step_count = sum(rendered.count(f'æ­¥é©Ÿ {i}') for i in range(1, 20))
                if result.step_count == 0:
                    result.step_count = sum(rendered.count(f'Step {i}') for i in range(1, 20))
                
                # æª¢æŸ¥å‹•è©ä½¿ç”¨
                verb_count = sum(rendered.count(verb) for verb in action_verbs)
                result.has_action_verbs = verb_count >= 3
                
                # è¨ˆç®—çµæ§‹è©•åˆ† (0-100)
                score = 0
                notes = []
                
                # æœ‰æ¸…æ™°æ­¥é©Ÿ +40
                if result.has_clear_steps:
                    score += 40
                    notes.append("æœ‰æ­¥é©Ÿæ¨™è¨˜")
                else:
                    notes.append("ç¼ºæ­¥é©Ÿæ¨™è¨˜")
                
                # æ­¥é©Ÿæ•¸é‡ 3-7 å€‹ +30
                if 3 <= result.step_count <= 7:
                    score += 30
                    notes.append(f"æ­¥é©Ÿæ•¸é©ä¸­({result.step_count})")
                elif result.step_count > 0:
                    score += 15
                    notes.append(f"æ­¥é©Ÿæ•¸æ¬ ä½³({result.step_count})")
                else:
                    notes.append("ç„¡æ˜ç¢ºæ­¥é©Ÿ")
                
                # æœ‰å‹•è© +30
                if result.has_action_verbs:
                    score += 30
                    notes.append("æœ‰å‹•ä½œå‹•è©")
                else:
                    notes.append("ç¼ºå‹•ä½œå‹•è©")
                
                result.structure_score = score
                result.structure_notes = "; ".join(notes)
                
                # åˆ¤æ–·çµæ§‹å“è³ª
                if score >= 70:
                    status = "âœ… è‰¯å¥½"
                    good_structure += 1
                else:
                    status = "âš ï¸  å¾…æ”¹å–„"
                    poor_structure += 1
                
                # æ‰“å°çµæœ
                step_str = str(result.step_count) if result.step_count > 0 else '-'
                verb_str = 'âœ“' if result.has_action_verbs else 'âœ—'
                print(f"{result.name:<35} {step_str:<8} {verb_str:<8} {score:<8} {status}")
                
            except Exception as e:
                result.structure_notes = f"æª¢æŸ¥å¤±æ•—: {e}"
                print(f"{result.name:<35} {'N/A':<8} {'N/A':<8} {'N/A':<8} âŒ ERROR")
        
        # çµ±è¨ˆ
        render_success_count = len([r for r in self.results if r.render_success])
        
        print("\n" + "="*80)
        print("ğŸ“Š çµæ§‹çµ±è¨ˆ")
        print("="*80)
        print(f"è‰¯å¥½çµæ§‹: {good_structure}/{render_success_count} ({good_structure/render_success_count*100:.1f}%)")
        print(f"å¾…æ”¹å–„: {poor_structure}/{render_success_count} ({poor_structure/render_success_count*100:.1f}%)")
        
        return {
            'good_structure': good_structure,
            'poor_structure': poor_structure,
            'total': render_success_count
        }
    
    # ==================== æ¸¬è©¦ 6: PromptManager æ•´åˆ ====================
    
    def test_prompt_manager_integration(self) -> Tuple[bool, str]:
        """
        æ¸¬è©¦ 6: PromptManager æ•´åˆ
        
        Returns:
            (æˆåŠŸèˆ‡å¦, è¨Šæ¯)
        """
        print("\n" + "="*80)
        print("æ¸¬è©¦ 6: PromptManager æ•´åˆæ¸¬è©¦")
        print("="*80)
        
        try:
            # å‰µå»º PromptManager
            manager = PromptManager()
            self.manager = manager
            
            # æª¢æŸ¥è¨»å†Šæ•¸é‡
            registered_count = len(manager.get_prompt_names())
            expected_count = 19
            
            print(f"\nè¨»å†Š Prompts æ•¸é‡: {registered_count}")
            print(f"é æœŸæ•¸é‡: {expected_count}")
            
            if registered_count == expected_count:
                print(f"âœ… PromptManager æ•´åˆæˆåŠŸ")
                
                # æ¸¬è©¦ list_prompts
                prompts_list = manager.list_prompts()
                print(f"\nlist_prompts() è¿”å›: {len(prompts_list)} å€‹ prompts")
                
                # æ¸¬è©¦ get_prompt
                sample_prompt = manager.get_prompt('basic_help_guide')
                if sample_prompt:
                    print(f"âœ… get_prompt() æˆåŠŸç²å– prompt")
                else:
                    print(f"âš ï¸  get_prompt() ç„¡æ³•ç²å– prompt")
                
                return True, "PromptManager æ•´åˆæ¸¬è©¦é€šé"
            else:
                msg = f"âŒ è¨»å†Šæ•¸é‡ä¸ç¬¦: é æœŸ {expected_count}, å¯¦éš› {registered_count}"
                print(msg)
                return False, msg
                
        except Exception as e:
            msg = f"âŒ PromptManager æ•´åˆå¤±æ•—: {e}"
            print(msg)
            return False, msg
    
    # ==================== ç”Ÿæˆå ±å‘Š ====================
    
    def generate_report(self, output_path: str = None) -> str:
        """
        ç”Ÿæˆè¨ºæ–·å ±å‘Š
        
        Args:
            output_path: å ±å‘Šè¼¸å‡ºè·¯å¾‘ï¼Œé»˜èªç‚º docs/PROMPTS_DIAGNOSTIC_REPORT.md
            
        Returns:
            å ±å‘Šå…§å®¹
        """
        if output_path is None:
            output_path = os.path.join(
                os.path.dirname(__file__), 
                '..', 
                'docs', 
                'PROMPTS_DIAGNOSTIC_REPORT.md'
            )
        
        # çµ±è¨ˆæ•¸æ“š
        total = len(self.results)
        import_success = len([r for r in self.results if r.import_success])
        instantiate_success = len([r for r in self.results if r.instantiate_success])
        render_success = len([r for r in self.results if r.render_success])
        length_pass = len([r for r in self.results if r.length_status == "PASS"])
        length_fail = len([r for r in self.results if r.length_status == "FAIL"])
        
        # åˆ†é¡å•é¡Œ
        p0_issues = [r for r in self.results if not r.import_success or not r.instantiate_success or not r.render_success]
        p1_issues = [r for r in self.results if r.render_success and r.length_over > r.length_standard * 0.5]
        p2_issues = [r for r in self.results if r.render_success and 0 < r.length_over <= r.length_standard * 0.5]
        p3_issues = [r for r in self.results if r.render_success and r.structure_score < 70]
        
        # ç”Ÿæˆå ±å‘Š
        report = f"""# Prompts è¨ºæ–·å ±å‘Š ğŸ”

**FHL Bible MCP Server - Prompts å„ªåŒ–è¨ºæ–·**

---

## åŸ·è¡Œè³‡è¨Š

- **åŸ·è¡Œæ™‚é–“**: {self._get_current_time()}
- **æ¸¬è©¦ç‰ˆæœ¬**: 1.0
- **æ¸¬è©¦ Prompts æ•¸é‡**: {total}
- **æ¸¬è©¦é …ç›®**: 6 é …ï¼ˆå°å…¥ã€å¯¦ä¾‹åŒ–ã€æ¸²æŸ“ã€é•·åº¦ã€çµæ§‹ã€æ•´åˆï¼‰

---

## ğŸ“Š æ¸¬è©¦çµæœç¸½è¦½

### åŸºæœ¬æ¸¬è©¦

| æ¸¬è©¦é …ç›® | æˆåŠŸ | å¤±æ•— | æˆåŠŸç‡ |
|---------|------|------|--------|
| **å°å…¥æ¸¬è©¦** | {import_success}/{total} | {total - import_success}/{total} | {import_success/total*100:.1f}% |
| **å¯¦ä¾‹åŒ–æ¸¬è©¦** | {instantiate_success}/{total} | {total - instantiate_success}/{total} | {instantiate_success/total*100:.1f}% |
| **æ¸²æŸ“æ¸¬è©¦** | {render_success}/{total} | {total - render_success}/{total} | {render_success/total*100:.1f}% |

### é•·åº¦åˆ†æ

| æ¸¬è©¦é …ç›® | é€šé | æœªé€šé | é€šéç‡ |
|---------|------|--------|--------|
| **é•·åº¦æ¨™æº–** | {length_pass}/{render_success} | {length_fail}/{render_success} | {length_pass/render_success*100 if render_success > 0 else 0:.1f}% |

**æ¨™æº–**: åŸºç¤ < 500 å­—ï¼Œè®€ç¶“ < 700 å­—ï¼Œç ”ç¶“ < 800 å­—ï¼Œç‰¹æ®Š < 900 å­—ï¼Œé€²éš < 1000 å­—

---

## ğŸ“‹ è©³ç´°åˆ†æ

### é•·åº¦åˆ†æè¡¨

| Prompt | é¡åˆ¥ | ç•¶å‰é•·åº¦ | æ¨™æº– | è¶…æ¨™ | ç‹€æ…‹ |
|--------|------|----------|------|------|------|
"""
        
        # æ·»åŠ æ¯å€‹ prompt çš„è©³ç´°è³‡æ–™
        for result in sorted(self.results, key=lambda r: r.length_over if r.render_success else 0, reverse=True):
            if result.render_success:
                status_icon = "âœ…" if result.length_status == "PASS" else "âŒ"
                over_str = f"+{result.length_over}" if result.length_over > 0 else "-"
                report += f"| `{result.name}` | {result.category} | {result.rendered_length} | {result.length_standard} | {over_str} | {status_icon} {result.length_status} |\n"
            else:
                report += f"| `{result.name}` | {result.category} | N/A | {result.length_standard} | N/A | âŒ ERROR |\n"
        
        report += f"""
### çµæ§‹åˆ†æè¡¨

| Prompt | æ­¥é©Ÿæ•¸ | çµæ§‹è©•åˆ† | ç‹€æ…‹ | å‚™è¨» |
|--------|--------|----------|------|------|
"""
        
        for result in sorted(self.results, key=lambda r: r.structure_score if r.render_success else 0):
            if result.render_success:
                status_icon = "âœ…" if result.structure_score >= 70 else "âš ï¸"
                step_str = str(result.step_count) if result.step_count > 0 else "-"
                report += f"| `{result.name}` | {step_str} | {result.structure_score}/100 | {status_icon} | {result.structure_notes} |\n"
            else:
                report += f"| `{result.name}` | - | - | âŒ | æ¸²æŸ“å¤±æ•— |\n"
        
        report += f"""
---

## ğŸš¨ å•é¡Œåˆ†é¡

### P0: è¼‰å…¥/æ¸²æŸ“å¤±æ•—ï¼ˆå¿…é ˆç«‹å³ä¿®å¾©ï¼‰

**æ•¸é‡**: {len(p0_issues)} å€‹

"""
        
        if p0_issues:
            for result in p0_issues:
                report += f"""#### `{result.name}`

- **é¡åˆ¥**: {result.category}
- **å•é¡Œ**:
"""
                if not result.import_success:
                    report += f"  - âŒ å°å…¥å¤±æ•—: `{result.import_error}`\n"
                if not result.instantiate_success:
                    report += f"  - âŒ å¯¦ä¾‹åŒ–å¤±æ•—: `{result.instantiate_error}`\n"
                if not result.render_success:
                    report += f"  - âŒ æ¸²æŸ“å¤±æ•—: `{result.render_error}`\n"
                report += "\n"
        else:
            report += "âœ… ç„¡ P0 å•é¡Œ\n\n"
        
        report += f"""### P1: åš´é‡è¶…é•·ï¼ˆé«˜å„ªå…ˆç´šï¼‰

**æ•¸é‡**: {len(p1_issues)} å€‹

**æ¨™æº–**: è¶…éæ¨™æº– 50% ä»¥ä¸Š

"""
        
        if p1_issues:
            for result in sorted(p1_issues, key=lambda r: r.length_over, reverse=True):
                over_pct = (result.length_over / result.length_standard) * 100
                report += f"- [ ] **`{result.name}`**: {result.rendered_length} å­—ï¼ˆæ¨™æº– {result.length_standard}ï¼Œè¶…æ¨™ {result.length_over} å­—ï¼Œ+{over_pct:.0f}%ï¼‰\n"
            report += "\n"
        else:
            report += "âœ… ç„¡ P1 å•é¡Œ\n\n"
        
        report += f"""### P2: ä¸­åº¦è¶…é•·ï¼ˆä¸­å„ªå…ˆç´šï¼‰

**æ•¸é‡**: {len(p2_issues)} å€‹

**æ¨™æº–**: è¶…éæ¨™æº– 0-50%

"""
        
        if p2_issues:
            for result in sorted(p2_issues, key=lambda r: r.length_over, reverse=True):
                over_pct = (result.length_over / result.length_standard) * 100
                report += f"- [ ] **`{result.name}`**: {result.rendered_length} å­—ï¼ˆæ¨™æº– {result.length_standard}ï¼Œè¶…æ¨™ {result.length_over} å­—ï¼Œ+{over_pct:.0f}%ï¼‰\n"
            report += "\n"
        else:
            report += "âœ… ç„¡ P2 å•é¡Œ\n\n"
        
        report += f"""### P3: çµæ§‹å¾…å„ªåŒ–ï¼ˆä½å„ªå…ˆç´šï¼‰

**æ•¸é‡**: {len(p3_issues)} å€‹

**æ¨™æº–**: çµæ§‹è©•åˆ† < 70

"""
        
        if p3_issues:
            for result in sorted(p3_issues, key=lambda r: r.structure_score):
                report += f"- [ ] **`{result.name}`**: è©•åˆ† {result.structure_score}/100ï¼ˆ{result.structure_notes}ï¼‰\n"
            report += "\n"
        else:
            report += "âœ… ç„¡ P3 å•é¡Œ\n\n"
        
        report += f"""---

## ğŸ“ˆ çµ±è¨ˆæ•¸æ“š

### æŒ‰é¡åˆ¥çµ±è¨ˆ

"""
        
        # æŒ‰é¡åˆ¥çµ±è¨ˆ
        category_summary = {}
        for result in self.results:
            if result.category not in category_summary:
                category_summary[result.category] = {
                    'total': 0,
                    'render_success': 0,
                    'length_pass': 0,
                    'length_fail': 0,
                    'total_length': 0,
                    'avg_length': 0,
                    'standard': result.length_standard
                }
            
            category_summary[result.category]['total'] += 1
            if result.render_success:
                category_summary[result.category]['render_success'] += 1
                category_summary[result.category]['total_length'] += result.rendered_length
                if result.length_status == "PASS":
                    category_summary[result.category]['length_pass'] += 1
                elif result.length_status == "FAIL":
                    category_summary[result.category]['length_fail'] += 1
        
        # è¨ˆç®—å¹³å‡å€¼
        for cat, stats in category_summary.items():
            if stats['render_success'] > 0:
                stats['avg_length'] = stats['total_length'] / stats['render_success']
        
        report += "| é¡åˆ¥ | æ•¸é‡ | æ¨™æº– | å¹³å‡é•·åº¦ | é€šé | æœªé€šé | é€šéç‡ |\n"
        report += "|------|------|------|----------|------|--------|--------|\n"
        
        for cat in ['basic', 'reading', 'study', 'special', 'advanced']:
            if cat in category_summary:
                stats = category_summary[cat]
                pass_rate = (stats['length_pass'] / stats['render_success'] * 100) if stats['render_success'] > 0 else 0
                report += f"| {cat} | {stats['total']} | {stats['standard']} | {stats['avg_length']:.0f} | {stats['length_pass']} | {stats['length_fail']} | {pass_rate:.1f}% |\n"
        
        report += f"""
### ç¸½é«”çµ±è¨ˆ

- **ç¸½ Prompts**: {total}
- **æ¸²æŸ“æˆåŠŸ**: {render_success}
- **é•·åº¦é€šé**: {length_pass}
- **é•·åº¦æœªé€šé**: {length_fail}
- **å¹³å‡é•·åº¦**: {sum(r.rendered_length for r in self.results if r.render_success) / render_success if render_success > 0 else 0:.0f} å­—
- **æœ€é•· Prompt**: {max((r for r in self.results if r.render_success), key=lambda r: r.rendered_length).name if render_success > 0 else 'N/A'} ({max((r.rendered_length for r in self.results if r.render_success), default=0)} å­—)

---

## ğŸ¯ é‡æ§‹å»ºè­°

### å„ªå…ˆç´šæ’åº

1. **ç«‹å³ä¿®å¾© P0** ({len(p0_issues)} å€‹)
   - ä¿®å¾©å°å…¥/å¯¦ä¾‹åŒ–/æ¸²æŸ“éŒ¯èª¤
   - ç¢ºä¿æ‰€æœ‰ prompts éƒ½èƒ½æ­£å¸¸è¼‰å…¥

2. **é«˜å„ªå…ˆç´š P1** ({len(p1_issues)} å€‹)
   - åš´é‡è¶…é•·çš„ prompts
   - éœ€è¦å¤§å¹…ç¸®æ¸›å…§å®¹ï¼ˆ50%+ è¶…æ¨™ï¼‰

3. **ä¸­å„ªå…ˆç´š P2** ({len(p2_issues)} å€‹)
   - ä¸­åº¦è¶…é•·çš„ prompts
   - éœ€è¦é©åº¦ç¸®æ¸›å…§å®¹ï¼ˆ0-50% è¶…æ¨™ï¼‰

4. **ä½å„ªå…ˆç´š P3** ({len(p3_issues)} å€‹)
   - çµæ§‹å¾…å„ªåŒ–çš„ prompts
   - æ”¹å–„æ­¥é©Ÿæ¸…æ™°åº¦å’Œå‹•è©ä½¿ç”¨

### é‡æ§‹ç­–ç•¥

**æ¼¸é€²å„ªåŒ–ï¼ˆStrategy Bï¼‰**:
1. Phase 1: ä¿®å¾© P0 å•é¡Œï¼ˆDay 1-2ï¼‰
2. Phase 2: é‡æ§‹ P1 å•é¡Œï¼ˆDay 3-7ï¼‰
3. Phase 3: å„ªåŒ– P2 å•é¡Œï¼ˆDay 8-10ï¼‰
4. Phase 4: æ”¹å–„ P3 å•é¡Œï¼ˆDay 11-12ï¼‰

### ç›®æ¨™

- âœ… **100% è¼‰å…¥æˆåŠŸç‡**
- âœ… **100% é•·åº¦åˆè¦**
- âœ… **90%+ çµæ§‹è‰¯å¥½**
- âœ… **å¹³å‡é•·åº¦ < 700 å­—**

---

## ğŸ“ ä¸‹ä¸€æ­¥è¡Œå‹•

1. **å¯©é–±å ±å‘Š**: ç¢ºèªå•é¡Œåˆ†é¡å’Œå„ªå…ˆç´š
2. **æº–å‚™é‡æ§‹**: å‰µå»ºé‡æ§‹æ¨¡æ¿å’Œæª¢æŸ¥æ¸…å–®
3. **é–‹å§‹ä¿®å¾©**: å¾ P0 å•é¡Œé–‹å§‹ï¼Œé€æ­¥è§£æ±º
4. **æ¸¬è©¦é©—è­‰**: æ¯æ¬¡é‡æ§‹å¾Œé‹è¡Œè¨ºæ–·æ¸¬è©¦
5. **æ›´æ–°æ–‡æª”**: å°‡è©³ç´°èªªæ˜ç§»è‡³ä½¿ç”¨æŒ‡å—

---

**å ±å‘Šç”Ÿæˆæ™‚é–“**: {self._get_current_time()}  
**å ±å‘Šç‰ˆæœ¬**: 1.0  
**ç‹€æ…‹**: è¨ºæ–·å®Œæˆï¼Œå¾…é‡æ§‹

---

Made with â¤ï¸ for better Prompts ğŸš€
"""
        
        # å¯«å…¥æª”æ¡ˆ
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nâœ… è¨ºæ–·å ±å‘Šå·²ç”Ÿæˆ: {output_path}")
        
        return report
    
    def _get_current_time(self) -> str:
        """ç²å–ç•¶å‰æ™‚é–“å­—ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


# ==================== Pytest æ¸¬è©¦å‡½æ•¸ ====================

@pytest.fixture(scope="module")
def diagnostics():
    """å‰µå»ºè¨ºæ–·æ¸¬è©¦å¯¦ä¾‹"""
    return PromptsDiagnostics()


def test_1_import_all_prompts(diagnostics):
    """æ¸¬è©¦ 1: å°å…¥æ¸¬è©¦"""
    success, fail, results = diagnostics.test_import_all_prompts()
    assert success > 0, "è‡³å°‘æ‡‰è©²æœ‰ä¸€å€‹ prompt å°å…¥æˆåŠŸ"


def test_2_instantiate_all_prompts(diagnostics):
    """æ¸¬è©¦ 2: å¯¦ä¾‹åŒ–æ¸¬è©¦"""
    success, fail = diagnostics.test_instantiate_all_prompts()
    assert success > 0, "è‡³å°‘æ‡‰è©²æœ‰ä¸€å€‹ prompt å¯¦ä¾‹åŒ–æˆåŠŸ"


def test_3_render_all_prompts(diagnostics):
    """æ¸¬è©¦ 3: æ¸²æŸ“æ¸¬è©¦"""
    success, fail = diagnostics.test_render_all_prompts()
    assert success > 0, "è‡³å°‘æ‡‰è©²æœ‰ä¸€å€‹ prompt æ¸²æŸ“æˆåŠŸ"


def test_4_prompts_length_analysis(diagnostics):
    """æ¸¬è©¦ 4: é•·åº¦åˆ†æ"""
    stats = diagnostics.test_prompts_length_analysis()
    assert stats['total'] > 0, "æ‡‰è©²æœ‰å¯åˆ†æçš„ prompts"


def test_5_prompts_structure_check(diagnostics):
    """æ¸¬è©¦ 5: çµæ§‹æª¢æŸ¥"""
    stats = diagnostics.test_prompts_structure_check()
    assert stats['total'] > 0, "æ‡‰è©²æœ‰å¯æª¢æŸ¥çš„ prompts"


def test_6_prompt_manager_integration(diagnostics):
    """æ¸¬è©¦ 6: PromptManager æ•´åˆ"""
    success, msg = diagnostics.test_prompt_manager_integration()
    assert success, f"PromptManager æ•´åˆå¤±æ•—: {msg}"


def test_7_generate_report(diagnostics):
    """æ¸¬è©¦ 7: ç”Ÿæˆè¨ºæ–·å ±å‘Š"""
    report = diagnostics.generate_report()
    assert report and len(report) > 0, "å ±å‘Šæ‡‰è©²æœ‰å…§å®¹"
    assert "# Prompts è¨ºæ–·å ±å‘Š" in report, "å ±å‘Šæ‡‰è©²åŒ…å«æ¨™é¡Œ"


# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    print("ğŸ” FHL Bible MCP Server - Prompts è¨ºæ–·æ¸¬è©¦")
    print("=" * 80)
    
    diagnostics = PromptsDiagnostics()
    
    # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
    diagnostics.test_import_all_prompts()
    diagnostics.test_instantiate_all_prompts()
    diagnostics.test_render_all_prompts()
    diagnostics.test_prompts_length_analysis()
    diagnostics.test_prompts_structure_check()
    diagnostics.test_prompt_manager_integration()
    
    # ç”Ÿæˆå ±å‘Š
    print("\n" + "="*80)
    print("ç”Ÿæˆè¨ºæ–·å ±å‘Š...")
    print("="*80)
    diagnostics.generate_report()
    
    print("\nâœ… è¨ºæ–·æ¸¬è©¦å®Œæˆï¼")
    print("\nè«‹æŸ¥çœ‹ docs/PROMPTS_DIAGNOSTIC_REPORT.md ç²å–è©³ç´°å ±å‘Š")
